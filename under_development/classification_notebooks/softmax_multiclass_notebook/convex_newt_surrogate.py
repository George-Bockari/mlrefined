# convex_newt_surrogate.py is a toy wrapper to illustrate the path
# taken by Hessian descent (or Newton's method).  The steps are evaluated
# at the objective, and then plotted.  For the first 5 iterations the
# quadratic surrogate used to transition from point to point is also plotted.
# The plotted points on the objective turn from green to red as the
# algorithm converges (or reaches a maximum iteration count, preset to 50).
# The (convex) function here is
#
# g(w) = log(1 + exp(w^2))
#
# This file pairs with chapter 2 of the textbook "Machine Learning Refined" published by Cambridge University Press, 
# free for download at www.mlrefined.com

import numpy as np
import matplotlib.pyplot as plt
import time
from IPython import display

# define the cost function
def calculate_cost_value(w):
    g = np.log(1 + np.exp(w**2))
    return g

# define the gradient of the cost function
def calculate_gradient(w):
    grad = (2*np.exp(w**2)*w)/(np.exp(w**2) + 1)
    return grad

# define the hessian / 2nd derivative of the cost function
def calculate_hessian(w):
    hess = (2*np.exp(w**2)*(2*w**2 + np.exp(w**2) + 1))/(np.exp(w**2) + 1)**2
    return hess

# define the quadratic surrogate generated by Newton's method
def surrogate(y,x):
    g = calculate_cost_value(y)
    grad = calculate_gradient(y)
    hess = calculate_hessian(y)
    h = g + grad*(x - y) + 0.5*hess*(x - y)*(x - y)
    return h

# Newton's method function
def newtons_method(w,max_its):
    # container for newton's method path
    w_history = []
    w_history.append(w)
    
    # main newton's method loop
    for k in range(0,max_its):
        # compute gradient
        grad = calculate_gradient(w)
        
        # compute hessian
        hess = calculate_hessian(w)
        
        # take newton step
        w = w - grad/hess

        # store current weights
        w_history.append(w)

    return w_history

# plot the cost function
def make_function(ax):
    # plot the function
    s = np.linspace(-1.1,1.1,200)
    t = np.log(1 + np.exp(s**2))
    ax.plot(s,t,'-k',linewidth = 2)

    # pretty the figure up
    ax.set_xlim(-1.1,1.1)
    ax.set_ylim(0.6,1.5)
    ax.set_xlabel('$w$',fontsize=20,labelpad = 20)
    ax.set_ylabel('$g(w)$',fontsize=20,rotation = 0,labelpad = 25)

# plot steps of Newton's method
def plot_steps_with_surrogate(w_path):
    # make figure to update
    fig = plt.figure(figsize = (12,5))
    ax1 = fig.add_subplot(111)
    
    # make cost function path based on gradient descent steps (in w_path)
    g_path = []
    for i in range(0,len(w_path)):
        w = w_path[i]
        g_path.append(calculate_cost_value(w))
        
    # plot costs function
    make_function(ax1)  
    display.clear_output(wait=True)
    display.display(plt.gcf()) 
    
    # colors for points
    s = np.linspace(1/len(g_path),1,len(g_path))
    s.shape = (len(s),1)
    colorspec = np.concatenate((s,np.flipud(s)),1)
    colorspec = np.concatenate((colorspec,np.zeros((len(s),1))),1)

    # plot initial point
    ax1.plot(w_path[0],g_path[0],'o',markersize = 12, color = colorspec[0,:], markerfacecolor = colorspec[0,:])
    display.clear_output(wait=True)
    display.display(plt.gcf())     
    
    # plot a tracer on this first point just for visualization purposes
    t = np.linspace(-0.5,g_path[0],100)
    s = w_path[0]*np.ones((100))
    ax1.plot(s,t,'--k')
    display.clear_output(wait=True)
    display.display(plt.gcf()) 
    time.sleep(2)

    # plot first quadratic surrogate
    s_range = 3  # range over which to show the linear surrogate
    s = np.linspace(w_path[0]-s_range,w_path[0]+s_range,10000)
    t = surrogate(w_path[0],s)
    h, = ax1.plot(s,t,'m',linewidth = 2)
    display.clear_output(wait=True)
    display.display(plt.gcf()) 
    time.sleep(1)
    
    # plot minimum of quadratic surrogate
    ind = np.argmin(t)
    x_mark, = ax1.plot(s[ind],t[ind],'kx',markersize = 12,markeredgewidth = 3)
    display.clear_output(wait=True)
    display.display(plt.gcf()) 
    
    # loop over the remaining iterations, showing
    # - the quadratic surrogate at the first few steps
    # - color changing from green (start) to red (end) of gradient descent run
    for i in range(1,len(g_path)):
            # with the first few points plot the surrogates as well for illustration
            if i <= 1:
                time.sleep(2.5)

                # plot cost function evaluated at next newton's method step
                ax1.plot(w_path[i],g_path[i],'o',markersize = 12, color = colorspec[i-1,:], markerfacecolor = colorspec[i-1,:])
                display.clear_output(wait=True)
                display.display(plt.gcf()) 
                time.sleep(1)

                # remove old quadratic and stationary pt from drawing
                h.remove()
                x_mark.remove()
                display.clear_output(wait=True)
                display.display(plt.gcf()) 
                
                # draw new quadratic 
                s_range = 3
                s = np.linspace(w_path[i]-s_range,w_path[i]+s_range,10000)
                t = surrogate(w_path[i],s)
                h, = ax1.plot(s,t,'m',linewidth = 2)
                display.clear_output(wait=True)
                display.display(plt.gcf()) 
                time.sleep(1)
                
                # draw minimum / maximum of quadratic
                ind = np.argmin(t)
                x_mark, = ax1.plot(s[ind],t[ind],'kx',markersize = 12,markeredgewidth = 3)
                display.clear_output(wait=True)
                display.display(plt.gcf()) 
    
            # remove quadratic surrogate, point, etc.,
            if i == 1:
                time.sleep(2.5)
                ax1.plot(w_path[i+1],g_path[i+1],'o',markersize = 12, color = colorspec[i,:], markerfacecolor = colorspec[i,:])
                display.clear_output(wait=True)
                display.display(plt.gcf())
                
                # remove quadratic and pt
                time.sleep(1)
                h.remove()
                x_mark.remove()
                display.clear_output(wait=True)
                display.display(plt.gcf()) 
    
            # for later iterations just plot point so things don't get too visually cluttered
            if i >= 1: # just plot point so things don't get too cluttered
                time.sleep(0.01)
                ax1.plot(w_path[i],g_path[i],'o',markersize = 12, color = colorspec[i-1,:], markerfacecolor = colorspec[i-1,:])
                display.clear_output(wait=True)
                display.display(plt.gcf())      
                
            # color the final point red just for visualization purposes
            if i == len(g_path) - 1:
                t = np.linspace(-0.5,g_path[i],100)
                s = w_path[i]*np.ones((100))
                ax1.plot(s,t,'--k')
                display.clear_output(wait=True)
                display.display(plt.gcf()) 

# main function
def run(w0,max_its):
    # run newton's method with chosen initial w0 
    w_path = newtons_method(w0,max_its)
    
    # plot all of the newton's method steps evaluated at the cost function
    plot_steps_with_surrogate(w_path)
    display.clear_output(wait=True)
