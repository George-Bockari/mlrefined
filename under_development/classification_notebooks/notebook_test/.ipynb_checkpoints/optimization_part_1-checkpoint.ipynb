{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning: Algorithms and Applications "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical optimization: part 1\n",
    "\n",
    "**Mathematical optimization** - Every learning problem has parameters that must be tuned properly to ensure optimal learning. For example, there are two parameters that must be properly tuned in the case linear regression (with one dimensional input): the slope and intercept of the linear model. Likewise with linear regression and higher dimensional input, as well as classification, we must properly tune the intercept and normal vector to the fitting hyperplane. Irregardless, the tuning of these parameters is accomplished by a set of tools known collectively as mathematical optimization. Mathematical optimization is the formal study of how to properly tune (machine learning) models, and is applied for similar reasons in a variety of other fields outside of machine learning (including operations, logistics, physics, etc.,).\n",
    "\n",
    "Here lets motivate the idea of numerical optimization here - its something that every machine learning problem involves, and it can be described in advance / along the way.  These images from the book say it all.\n",
    "\n",
    "![image](files/bigpicture_regression_optimization.png) \n",
    "\n",
    "The same holds with classification.\n",
    "\n",
    "![image](files/bigpicture_classification_optimization.png) \n",
    "\n",
    "\n",
    "\n",
    "##  1.1  Our first foray into numerical optimization\n",
    "\n",
    "So, we want to minimize, or equivalently find a \\emph{minimum }point\n",
    "of, a (cost) function. You may remember that such a task is actually\n",
    "a fundamental subject of basic calculus, which we will now review.\n",
    "For now lets simplify notation a bit and discuss this concept for\n",
    "a general function $g\\left(w\\right)$ which take in a scalar input\n",
    "$w$ (we'll generalize to vector input functions afterwards).\n",
    "\n",
    "Remember that geometrically speaking the derivative of $g\\left(w\\right)$\n",
    "at a particular point $v$, which we will write as $ $$g^{\\prime}\\left(v\\right)$, is the \\emph{slope} of the best *linear approximation* to $g$ at that point (This best linear approximation is referred to as the *first order Taylor series approximation*). This concept is visualized in Figure \\ref{fig:Taylor-approximations}. \n",
    "\n",
    "Now, examining any differentiable $g\\left(w\\right)$ we can easily\n",
    "conclude that minima are naturally located where the slope of the\n",
    "linear approximation is zero, i.e., at points $w=v$ where $g^{\\prime}\\left(v\\right)=0$. This gives us a simple looking rule for identifying minima of a scalar input function $g$ called the \\emph{first order condition }- just find a point $v$ satisfying $g^{\\prime}\\left(v\\right)=0$. However closer examination shows that often a) other non-minima points also satisfy this rule and b) employing this rule 'by hand' can be extremely challenging. But one thing at a time - if we just look more closely at the wiggly function $g$ in the left panel of Figure \\ref{fig:stationary-pts-illustrated} we can see that the other points satisfying this condition include maxima and saddle points of a function as well. Collectively these minima, maxima, and saddle points\\footnote{Once a critical point $w^{\\ast}$ is found via the first order condition, how do we tell if it is a minima, maxima, or saddle point? There is something called the \\emph{second derivative test} that can be used to determine this. In practice, however, we will see that the numerical algorithms based off of the first order condition } are often referred to as *stationary* or *critical points*. \n",
    "\n",
    "\\begin{minibox}\n",
    "a stationary point for some general function can be a minimum, maximum,\n",
    "or saddle point \n",
    "\\end{minibox}\n",
    "\n",
    "![image](files/convex.png) \n",
    "\n",
    "\n",
    "* $g\\left(w\\right)=w^{2}$ : A simple quadratic or parabola that we know has a unique minimum at $w=0$. Lets see how the first order condition finds this point. Setting the derivative to zero we have $g^{\\prime}\\left(w\\right)=2w=0$, and the only value for $w$ making this true is indeed $w=0$.\n",
    "\n",
    "\n",
    "* $g\\left(w\\right)=w^{3}$ : A simple cubic that goes off to positive and negative infinity, and has one saddle point at $w=0$. Lets check that the first order condition gives us this too. We have that $g^{\\prime}\\left(w\\right)=3w^{2}=0$\n",
    "and, again, the only value of $w$ satisfying this is $w=0$\n",
    "\n",
    "\n",
    "* $g\\left(w\\right)=\\mbox{sin}\\left(w\\right)$ : - exercise\n",
    "\n",
    "One important thing to note: for convex functions, i.e., functions that look like upward facing cups and have nonnegative curvature everywhere like the simple quadratic $g\\left(w\\right)=w^{2}$, the only critical points are minima themselves.\n",
    "\n",
    "\\begin{box}\n",
    "all stationary points of a convex function are minima\n",
    "\\end{box}\n",
    "\n",
    "Everything we have said so far holds precisely for the vector input case as well. The only thing that changes are the formulae, and then only slightly. \n",
    "\n",
    "Remember that the derivative generalizes in the case of vector input\n",
    "functions $g\\left(\\mathbf{w}\\right)$ to the \"gradient\", or an $N\\times1$ vector of partial derivatives of $g\\left(\\mathbf{w}\\right)$ as \n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla g\\left(\\mathbf{w}\\right)=\\left[\\begin{array}{c}\n",
    "\\frac{\\partial}{\\partial w_{1}}g\\\\\n",
    "\\frac{\\partial}{\\partial w_{2}}g\\\\\n",
    "\\vdots\\\\\n",
    "\\frac{\\partial}{\\partial w_{N}}g\n",
    "\\end{array}\\right].\n",
    "\\end{equation}\n",
    "\n",
    "In complete analogy to the one dimensional input case where we found stationary points by finding those $v$ where $g^{\\prime}\\left(v\\right)=0$, the first order condition for recovering stationary points of $g\\left(\\mathbf{w}\\right)$\n",
    "has us find those $\\mathbf{v}$ that have zero gradient, i.e., $\\nabla g\\left(\\mathbf{v}\\right)=\\mathbf{0}_{N\\times1}$.\n",
    "Notice how indeed the condition $\\nabla g\\left(\\mathbf{v}\\right)=\\mathbf{0}_{N\\times1}$\n",
    "reduces to $g^{\\prime}\\left(v\\right)=0$ when $N=1$. However notice that in the case where $N>1$ this first order condition is a *system* of $N$ equations i.e., \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{array}{c}\n",
    "\\frac{\\partial}{\\partial w_{1}}g=0,\\\\\n",
    "\\frac{\\partial}{\\partial w_{2}}g=0,\\\\\n",
    "\\vdots\\\\\n",
    "\\frac{\\partial}{\\partial w_{N}}g=0.\n",
    "\\end{array}\n",
    "\\end{equation}\n",
    "\n",
    "The catch here, which will lead us to discuss numerical methods for minimizing cost functions, is that in general these systems of equations cannot be solved in closed form (using 'pencil and paper'). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I think at this point we can use a few of the problems from the appendix + chap exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
